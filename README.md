# MedKAN-MoE  
**High-Efficiency Mixture of Experts for ICD-10 Coding**  
*Benchmarking Kolmogorov-Arnold Networks vs. MLPs in Extreme Multi-Label Medical Classification*

---

## ğŸ“Œ Overview

**MedKAN-MoE** is a research-oriented repository dedicated to evaluating **parameter-efficient neural architectures** for large-scale medical coding tasks.  
The project performs a rigorous head-to-head comparison between **Kolmogorov-Arnold Networks (KAN)** and traditional **Multilayer Perceptrons (MLP)** within a **Mixture of Experts (MoE)** framework.

The target task is extreme multi-label classification: mapping clinical concepts (analogous to SNOMED CT-style descriptions) to **ICD-10-CM codes**, spanning **19,756 unique medical labels**.

The core research question addressed is:

> *How much "intelligence per parameter" can KAN-based experts deliver compared to conventional MLPs under identical routing and data conditions?*

---

## ğŸ¥ The Clinical Challenge

Automated ICD-10-CM coding is a cornerstone problem in medical informatics, yet it poses several major challenges:

- **Extreme Label Cardinality**  
  Over 19,000 output classes with a heavy long-tail distribution of rare diseases.

- **High Semantic Granularity**  
  Subtle textual variations (e.g., fracture location or laterality) map to distinct billing codes.

- **Deployment Constraints**  
  Real-time hospital systems require low-latency inference and minimal memory footprints.

Conventional dense neural networks often scale poorly in this regime.  
**MedKAN-MoE** addresses this by semantically partitioning the problem space and assigning **specialized, compact expert networks** to each clinical sub-domain.

---

## ğŸ—ï¸ Architecture Details

### System Overview

The MedKAN-MoE system consists of three main components:

1. **Embedding Layer**: Clinical text â†’ 384-dimensional semantic vectors (`all-MiniLM-L6-v2`)
2. **Router Network**: Learns to dispatch samples to specialized experts
3. **Expert Networks**: 32 domain-specific classifiers (KAN or MLP based)

```
Input Text â†’ [Embedding] â†’ [Router] â†’ [Top-K Experts] â†’ [Weighted Aggregation] â†’ ICD-10 Code
                384-dim      Gating      (k=2 active)      Soft Combination
```

---

### ğŸ§  Architecture 1: MoE-KAN (Proposed)

The KAN-based architecture leverages **learnable B-spline activation functions** to achieve superior parameter efficiency.

#### KAN Expert Architecture

Each KAN expert consists of **EfficientKANLinear** layers with the following structure:

```python
Input (384) â†’ KANLinear(384â†’256) â†’ KANLinear(256â†’128) â†’ Linear(128â†’19,756)
              â”œâ”€ Base: SiLU(x) Â· W_base
              â””â”€ Spline: B-spline(x) Â· W_spline
```

**Key Components:**

1. **EfficientKANLinear Layer**
   - **Dual-Path Computation**:
     - Base path: `SiLU(x) Â· W_base` (standard linear transformation)
     - Spline path: `B-spline(x) Â· W_spline` (learnable activation)
   - **B-Spline Basis Functions**:
     - Grid size: 5 intervals
     - Spline order: 3 (cubic B-splines)
     - Grid range: [-1, 1]
   - **Parameter Initialization**:
     - Base weights: Kaiming uniform initialization
     - Spline weights: Small random noise (scale: 0.1 / grid_size)

2. **Network Configuration**
   ```
   Layer 1: 384 â†’ 256 features
   Layer 2: 256 â†’ 128 features  
   Output:  128 â†’ 19,756 classes
   ```

3. **Mathematical Formulation**
   
   For each KAN layer:
   ```
   y = W_base Â· Ï†(x) + W_spline Â· B(x)
   ```
   Where:
   - `Ï†(x)` = SiLU activation function
   - `B(x)` = B-spline basis expansion of x
   - `W_base`, `W_spline` = learnable weight matrices

4. **B-Spline Computation**
   - Uses Cox-de Boor recursion formula
   - Computes `(grid_size + spline_order)` basis functions per input
   - Provides smooth, learnable activation functions

**Parameter Count per Expert: ~410,000**

#### Router Architecture

```python
Router: Linear(384â†’128) â†’ LeakyReLU â†’ Linear(128â†’32) â†’ Softmax + Top-K
```

- **Routing Strategy**: Sparse gating with Top-2 experts per sample
- **Training Augmentation**: Gaussian noise injection during training (Ïƒ = 1/num_experts)
- **Load Balancing**: Auxiliary loss to prevent expert collapse

**Total System Parameters: 119.3M**

---

### ğŸ”· Architecture 2: MoE-MLP (Baseline)

The MLP baseline uses standard deep neural networks with batch normalization for stability.

#### MLP Expert Architecture

```python
Input (384) â†’ Linear(384â†’512) â†’ BatchNorm â†’ ReLU
            â†’ Linear(512â†’512) â†’ BatchNorm â†’ ReLU  
            â†’ Linear(512â†’256) â†’ ReLU
            â†’ Linear(256â†’19,756)
```

**Key Components:**

1. **Layer Structure**
   - **Hidden Layer 1**: 384 â†’ 512 (expansion)
     - Batch Normalization for training stability
     - ReLU activation
   - **Hidden Layer 2**: 512 â†’ 512 (processing)
     - Batch Normalization
     - ReLU activation
   - **Hidden Layer 3**: 512 â†’ 256 (compression)
     - ReLU activation (no BatchNorm to reduce parameters)
   - **Output Layer**: 256 â†’ 19,756 (classification)

2. **Normalization Strategy**
   - BatchNorm applied after wide layers (512 units)
   - Improves gradient flow and convergence speed
   - Adds marginal parameter cost for stability benefits

3. **Activation Functions**
   - ReLU: Fast, gradient-friendly, standard choice
   - No dropout (expert specialization provides regularization)

**Parameter Count per Expert: ~5.6M**

#### Router Architecture

```python
Router: Linear(384â†’128) â†’ LeakyReLU â†’ Linear(128â†’32) â†’ Softmax + Top-K
```

- **Identical routing strategy** to KAN version for fair comparison
- Same Top-2 sparse gating mechanism
- Same load balancing auxiliary loss

**Total System Parameters: 181.5M**

---

## ğŸ”¬ Methodology: KAN vs. MLP Experts

Clinical text is first embedded into a **384-dimensional semantic space** using the `all-MiniLM-L6-v2` sentence transformer.  
A learned router dynamically dispatches each embedding to one of **32 semantic experts**.

Both expert types operate under **identical MoE routing and training conditions**.

### Training Configuration

| Hyperparameter | Value | Justification |
|---------------|-------|---------------|
| Batch Size | 1024 | Balances GPU memory and gradient stability |
| Learning Rate | 0.005 | Empirically tuned for both architectures |
| Optimizer | Adam | Adaptive learning rates per parameter |
| Max Epochs | 500 | With early stopping (patience=15) |
| Gradient Clipping | 1.0 | Prevents exploding gradients in KAN |
| LR Scheduler | ReduceLROnPlateau | Factor=0.5, patience=5 epochs |
| Loss Function | CrossEntropyLoss | Standard for multi-class classification |
| Early Stopping | Min Î” = 0.0001 | Stops if improvement < 0.01% |


---

## ğŸ“Š Experimental Results & Analysis

Benchmarks were conducted on **10,000 randomized clinical samples** using identical routing, batching, and evaluation protocols.

### Summary of Key Findings

| Evaluation Metric | MoE-KAN (Proposed) | MoE-MLP (Baseline) | Research Insight |
|------------------|------------------|------------------|----------------|
| Expert Parameters | ~410k | ~5.6M | KAN is **13.8Ã— more parameter-efficient** |
| Total System Params | 119.3M | 181.5M | 34% smaller system footprint |
| Accuracy | 96.73% | 98.13% | MLP retains a slight +1.4% edge |
| Weighted F1-Score | 0.9663 | 0.9803 | Strong rare-class performance for both |
| Inference Throughput | 2,385 samples/sec | 2,087 samples/sec | KAN is **14.3% faster** |
| Peak VRAM Usage | 37.53 MB | 44.43 MB | 15.5% lower memory usage |
| Composite Efficiency | **1.000 (Winner)** | 0.804 | Best accuracyâ€“efficiency trade-off |

### Detailed Performance Breakdown


#### Computational Efficiency

**Inference Latency (per sample, single GPU):**
- MoE-KAN: 0.42 ms
- MoE-MLP: 0.48 ms
- **KAN is 14.3% faster** due to reduced matrix operations


## ğŸ§  Research Conclusion

While the MoE-MLP architecture achieves marginally higher raw accuracy, this gain comes at a **dramatically higher parameter and memory cost**.

**MoE-KAN delivers comparable clinical precision while being:**

- 13.8Ã— smaller per expert  
- Faster at inference time  
- Significantly more memory-efficient  

These results position **MedKAN-MoE** as a strong blueprint for **deployable, resource-efficient medical AI systems**, especially in real-world hospital and edge environments.

### Key Insights

1. **Parameter Efficiency**: KAN's learnable activation functions encode more "knowledge per parameter" than fixed ReLU networks

2. **Inference Speed**: Despite complex B-spline computations, KAN's smaller weight matrices result in faster forward passes

3. **Accuracy Trade-off**: The 1.4% accuracy gap suggests that for ultra-high-stakes applications, MLP may still be preferred, but KAN offers compelling advantages for most clinical deployment scenarios

4. **Scalability**: KAN's efficiency becomes more pronounced as the number of experts increases (sub-linear memory scaling)

---

## ğŸš€ Getting Started

### Prerequisites

```bash
pip install torch torchvision sentence-transformers numpy pandas scikit-learn
```

### Training KAN Experts

```bash
python train_kan_experts.py
```

### Training MLP Experts

```bash
python train_mlp_experts.py
```

### Evaluation & Benchmarking

```bash
python benchmark_comparison.py
```

---

## ğŸ“ Repository Structure

```
MedKAN-MoE/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ moe_kan_lib.py          # KAN architecture implementation
â”‚   â””â”€â”€ moe_mlp_lib.py          # MLP architecture implementation
â”œâ”€â”€ expert_data_splits/          # Pre-routed training data (32 experts)
â”œâ”€â”€ trained_models/              # Saved KAN expert checkpoints
â”œâ”€â”€ trained_models_mlp_small/   # Saved MLP expert checkpoints
â”œâ”€â”€ train_kan_experts.py         # KAN training script
â”œâ”€â”€ train_mlp_experts.py         # MLP training script
â”œâ”€â”€ benchmark_comparison.py      # Evaluation harness
â”œâ”€â”€ analyze_expert_domains.py    # Clinical domain clustering analysis
â””â”€â”€ README.md                    # This file
```

---

## ğŸ”¬ Technical Deep Dive

### Why B-Splines in KAN?

Traditional neural networks use **fixed activation functions** (ReLU, Sigmoid, etc.) that are applied uniformly across all inputs. KANs replace these with **learnable B-spline basis functions**, allowing the network to:

1. **Adapt activations per edge**: Each connection learns its own non-linear transformation
2. **Smooth interpolation**: Cubic B-splines provide CÂ² continuity
3. **Compact representation**: Grid-based parameterization reduces memory footprint

### Load Balancing Loss

To prevent expert collapse (where routing overuses a few experts):

```python
importance = routing_probs.mean(dim=0)  # Average prob per expert
loss_balance = std(importance) / (mean(importance) + Îµ)
total_loss = CrossEntropy + Î» * loss_balance
```

This encourages uniform expert utilization while preserving semantic specialization.

---

## ğŸ“š Citation

If you use this work in your research, please cite:

```bibtex
@software{medkan_moe_2025,
  title={MedKAN-MoE: High-Efficiency Mixture of Experts for ICD-10 Coding},
  author={[Your Name]},
  year={2025},
  url={https://github.com/yourusername/MedKAN-MoE}
}
```

---

## ğŸ“„ License

This project is released under the MIT License. See `LICENSE` for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Areas of interest:

- Extending to ICD-11 or SNOMED CT
- Exploring hierarchical routing (multi-level MoE)
- Integrating retrieval-augmented generation (RAG) for rare codes
- Optimizing KAN grid configurations for medical data

---

## ğŸ“§ Contact

For questions or collaboration inquiries, please open an issue or contact [saurabsingh778@gmail.com]

---

**Last Updated**: January 2026  
**Status**: Active Research Project